# Plan: Train and Ship the Semantic Endpointing Model (End-to-End)

## Goal

Replace the `EnergyVAD` fallback with a real ML model that distinguishes:
- `speaking` — user is talking
- `thinking_pause` — user paused but isn't done (mid-sentence breath, hesitation)
- `turn_complete` — user finished their thought
- `interrupt_intent` — user wants to interject

## Phase 1: Python Training Pipeline

### 1a. Set up environment
- Create venv in `training/`, install `requirements.txt`
- Verify torch, librosa, onnx, onnxruntime work on Python 3.14

### 1b. Source training data
- Write `training/data/download.py` to fetch a small subset of **LibriSpeech** (clean-100, ~100 hours of read English speech with utterance-level alignments)
- We only need ~5-10 hours for a small model. Download the `dev-clean` split (~5hrs, ~350MB)

### 1c. Generate frame-level labels
- Write `training/data/generate_labels.py`
- Process each audio file at 16kHz, 25ms frames, 10ms hop
- Derive labels from the audio structure:
  - Frames with speech energy above a threshold → `speaking`
  - Short silence gaps within an utterance (<2s) → `thinking_pause`
  - Silence at the end of an utterance → `turn_complete`
  - Synthetic interrupts: overlay two utterances, label the overlapping region as `interrupt_intent`
- Output: `.npz` files with feature arrays + label arrays per utterance

### 1d. Feature extraction
- Write `training/features/extract.py`
- Extract the 17-element vector per frame using librosa:
  - 13 MFCCs (`librosa.feature.mfcc`)
  - RMS energy (`librosa.feature.rms`)
  - Pitch (`librosa.pyin`)
  - Speech rate (energy envelope peak counting)
  - Pause duration (accumulated silence length)

### 1e. Model architecture
- Write `training/model.py`
- Small transformer encoder matching `configs/transformer_small.yaml`:
  - Input: `(batch, seq_len, 17)`
  - 2 transformer encoder layers, 4 heads, 64 hidden dim
  - Linear head → 4 class logits
  - Output: `(batch, seq_len, 4)`
- Also add a simpler **1D-CNN fallback** architecture for comparison (likely faster inference)

### 1f. Training
- Rewrite `training/train.py` (replace the stub)
- Load pre-extracted features from `.npz` files
- 80/20 train/val split
- Cross-entropy loss, AdamW optimizer, cosine LR scheduler
- Early stopping (patience=5)
- Save best checkpoint to `training/checkpoints/`
- Print per-class accuracy and confusion matrix

### 1g. ONNX export
- Rewrite `training/export.py` (replace the stub)
- Load best PyTorch checkpoint
- `torch.onnx.export()` with dynamic batch axis
- Quantize to float16 or int8 to hit <5MB
- Validate: run test inference with onnxruntime, compare to PyTorch output
- Copy final model to `models/utterance-v1.onnx`

## Phase 2: TypeScript SDK Integration

### 2a. Feature extraction in JS
- Implement the 3 stubbed features in `src/features/extractor.ts`:
  - **MFCCs**: Pre-emphasis → Hamming window → FFT → Mel filterbank → log → DCT
  - **Pitch**: Autocorrelation-based F0 estimation
  - **Speech rate**: Energy envelope peak counting
- These must produce values compatible with what the Python pipeline extracts (same scale, same normalization)

### 2b. ONNX Runtime Web integration
- Implement `src/model/onnx.ts`:
  - `load()`: Dynamically import `onnxruntime-web`, create `InferenceSession` from model URL or bundled path
  - `predict()`: Build input tensor from 17-element AudioFeatures, run inference, softmax output, return top label + confidence
  - Auto-detect WebAssembly vs WebGL execution provider
- Keep EnergyVAD as fallback when model fails to load

### 2c. Bundle the model
- Ship `models/utterance-v1.onnx` in the npm package (already in `files` array)
- The SDK loads it from the package when `modelPath: "bundled"`

## Phase 3: Testing

- Feature extractor tests: verify MFCC shape, pitch range, speech rate output
- ONNX model test: load model in Node.js via `onnxruntime-node`, verify output shape and label set
- Integration test: feed a known audio buffer through the full pipeline, verify events fire correctly
- Run `npm run build:sdk` and verify dist output

## File Changes

**New files:**
- `training/data/download.py`
- `training/data/generate_labels.py`
- `training/features/extract.py`
- `training/model.py`
- `models/utterance-v1.onnx` (output of training)

**Rewritten files (replace stubs):**
- `training/train.py`
- `training/export.py`

**Modified files:**
- `src/features/extractor.ts` — implement MFCCs, pitch, speech rate
- `src/model/onnx.ts` — implement ONNX Runtime Web inference

## Constraints
- Model must be <5MB (ONNX, quantized)
- Browser inference <50ms per frame
- Feature extraction must match between Python (training) and TypeScript (runtime)
- Must fall back gracefully to EnergyVAD if model fails to load
