# Plan: Train and Ship the Semantic Endpointing Model (End-to-End)

## Goal

Replace the `EnergyVAD` fallback with a real ML model that distinguishes:
- `speaking` — user is talking
- `thinking_pause` — user paused but isn't done (mid-sentence breath, hesitation)
- `turn_complete` — user finished their thought
- `interrupt_intent` — user wants to interject

## Design Decisions

| Decision | Choice |
|---|---|
| Model size ceiling | **<5MB** (ONNX, quantized) |
| Execution backend | **WASM default, WebGPU opt-in** |
| Training data | **Real conversational dataset** (Switchboard/Fisher, not LibriSpeech) |
| Inference cadence | **Batched every 100ms** (process window of frames, not per-frame) |
| Mobile Safari support | **Yes** — must work with inconsistent WASM SIMD on iOS |
| Feature parity (Python↔JS) | **Accept ~1-2% numerical drift**, train with noise augmentation |
| Architecture | **Hybrid** — 1D conv layers for local patterns + single attention layer for temporal context |
| Context window | **Fixed 1 second** — 100 frames at 10ms hop. Input shape `(batch, 100, 17)` → output `(batch, 4)` |
| npm distribution | **Bundled default, CDN as option** — `modelPath: "bundled"` or `modelPath: "https://..."` |
| Model variants | **Single model now**, add lite/full variants later based on real-world data |
| Accuracy target | **85%+ across all 4 classes** |
| Evaluation | **Playground + hand-labeled test set** (10-20 real conversational clips) |

## Phase 1: Python Training Pipeline

### 1a. Set up environment
- Create venv in `training/`, install `requirements.txt`
- Verify torch, librosa, onnx, onnxruntime work

### 1b. Source training data
- Write `training/data/download.py` to fetch **real conversational data**
- Primary: **Switchboard** corpus (telephone conversations with natural turn-taking)
  - ~260 hours of two-sided English telephone conversations
  - Has word-level alignments and speaker turn annotations
  - Download a subset (~10-20 hours) for v1
- Fallback: **Fisher** corpus or **CALLHOME** if Switchboard access is difficult
- The key advantage: real thinking pauses, real turn completions, real overlapping speech (interrupts) — not synthetic

### 1c. Generate frame-level labels
- Write `training/data/generate_labels.py`
- Process each audio file at 16kHz, 25ms frames, 10ms hop
- Derive labels from speaker turn annotations + audio analysis:
  - Frames with active speech from the target speaker → `speaking`
  - Silence gaps within a speaker's turn (between their words, <2s) → `thinking_pause`
  - Silence after a speaker's final word before the other speaker talks → `turn_complete`
  - Frames where both speakers overlap (one starts while the other is still talking) → `interrupt_intent`
- Output: `.npz` files with feature arrays + label arrays per conversation segment
- Apply noise augmentation to feature arrays during generation for robustness to Python↔JS feature drift

### 1d. Feature extraction
- Write `training/features/extract.py`
- Extract the 17-element vector per frame using librosa:
  - 13 MFCCs (`librosa.feature.mfcc`)
  - RMS energy (`librosa.feature.rms`)
  - Pitch (`librosa.pyin`)
  - Speech rate (energy envelope peak counting)
  - Pause duration (accumulated silence length)
- Add small Gaussian noise augmentation (σ=0.01-0.02) to training features to make model robust to JS feature extraction differences

### 1e. Model architecture
- Write `training/model.py`
- **Hybrid architecture** (conv + attention):
  - Input: `(batch, 100, 17)` — 1 second context window
  - 3× 1D conv blocks: Conv1d → BatchNorm → ReLU → optional pooling
    - Captures local temporal patterns (speech onset, silence boundaries)
  - 1× multi-head self-attention layer (4 heads, 64 dim)
    - Captures longer-range context (is this silence "mid-thought" or "done"?)
  - Global average pooling → Linear head → 4 class logits
  - Output: `(batch, 4)` — one classification per window
- Target: <5MB ONNX, <100ms inference in WASM on mobile

### 1f. Training
- Rewrite `training/train.py` (replace the stub)
- Load pre-extracted features from `.npz` files
- 80/20 train/val split, stratified by class
- Cross-entropy loss with class weights (thinking_pause and interrupt_intent are rarer)
- AdamW optimizer, cosine LR scheduler
- Early stopping (patience=5)
- Save best checkpoint to `training/checkpoints/`
- Print per-class accuracy, confusion matrix, and F1 scores
- Target: **85%+ accuracy across all 4 classes**

### 1g. ONNX export
- Rewrite `training/export.py` (replace the stub)
- Load best PyTorch checkpoint
- `torch.onnx.export()` with dynamic batch axis, fixed sequence length (100)
- Quantize to **int8** (optimal for WASM SIMD) with ONNX Runtime quantization tools
- Validate: run test inference with onnxruntime, compare to PyTorch output
- Verify model size <5MB
- Copy final model to `models/utterance-v1.onnx`

## Phase 2: TypeScript SDK Integration

### 2a. Feature extraction in JS
- Implement the 3 stubbed features in `src/features/extractor.ts`:
  - **MFCCs**: Pre-emphasis → Hamming window → FFT → Mel filterbank → log → DCT
  - **Pitch**: Autocorrelation-based F0 estimation (simplified YIN)
  - **Speech rate**: Energy envelope peak counting
- Accept ~1-2% numerical drift from librosa — the model is trained with noise augmentation to handle this
- Add a frame buffer to accumulate 100 frames (1 second) before triggering inference

### 2b. ONNX Runtime Web integration
- Implement `src/model/onnx.ts`:
  - `load()`: Dynamically import `onnxruntime-web`, create `InferenceSession`
    - Default execution provider: **WASM**
    - Optional: **WebGPU** when user opts in via config
    - Handle bundled model path (resolve from package) or user-provided URL
  - `predict()`: Build input tensor `(1, 100, 17)` from buffered frames, run inference, softmax output, return top label + confidence
  - Batch inference every **100ms** (not per-frame)
- Keep EnergyVAD as fallback when model fails to load
- Model loads async on `start()` — EnergyVAD runs immediately while model downloads

### 2c. Bundle the model
- Ship `models/utterance-v1.onnx` in the npm package (already in `files` array)
- Support two modes:
  - `modelPath: "bundled"` — loads from npm package (default)
  - `modelPath: "https://cdn.example.com/utterance-v1.onnx"` — loads from CDN

## Phase 3: Testing & Evaluation

### 3a. Unit tests
- Feature extractor tests: verify MFCC shape (13 coefficients), pitch range (50-500Hz), speech rate output
- ONNX model test: load model in Node.js via `onnxruntime-node`, verify output shape `(1, 4)` and label set

### 3b. Integration test
- Feed a known audio buffer through the full pipeline, verify events fire correctly
- Run `npm run build:sdk` and verify dist output

### 3c. Hand-labeled evaluation set
- Record 10-20 short conversational audio clips (2-person dialogue, single speaker monologue with pauses)
- Manually label turn boundaries, thinking pauses, and interrupts with timestamps
- Write `training/evaluate.py` to run the ONNX model on these clips and report per-class accuracy
- Target: **85%+ across all 4 classes** on the evaluation set

## File Changes

**New files:**
- `training/data/download.py` — fetch conversational dataset (Switchboard/Fisher)
- `training/data/generate_labels.py` — derive frame-level labels from turn annotations
- `training/features/extract.py` — Python feature extraction with noise augmentation
- `training/model.py` — hybrid conv + attention architecture
- `training/evaluate.py` — evaluation script for hand-labeled test set
- `models/utterance-v1.onnx` (output of training)

**Rewritten files (replace stubs):**
- `training/train.py`
- `training/export.py`

**Modified files:**
- `training/configs/transformer_small.yaml` → rename to `hybrid_v1.yaml`, update architecture
- `training/requirements.txt` — add any missing deps
- `src/features/extractor.ts` — implement MFCCs, pitch, speech rate + frame buffer
- `src/model/onnx.ts` — implement ONNX Runtime Web inference with WASM/WebGPU support

## Constraints
- Model must be <5MB (ONNX, int8 quantized)
- Inference batched every 100ms, must complete within that budget on mobile Safari WASM
- Feature extraction must be compatible between Python (training) and TypeScript (runtime) within ~1-2% tolerance
- Must fall back gracefully to EnergyVAD if model fails to load
- Must support mobile Safari (iOS WASM without reliable SIMD)
- 85%+ per-class accuracy on hand-labeled evaluation set
